## 云原生架构设计与应用

近年来，随着数字化转型不断深入，科技创新与业务发展不断融合，各行各业正在从大工业时代的固化范式进化成面向创新型组织与灵活型业务的崭新模式。在这一背景下，以容器和微服务架构为代表的云原生技术作为云计算服务的新模式，已经逐渐成为企业持续发展的主流选择。云原生架构是基于云原生技术的一组架构原则和设计模式的集合，旨在将云应用中的非业务代码部分进行最大化剥离，从而让云设施接管应用中原有的大量非功能特性(如弹性、韧性、安全、可观测性、灰度等)，使业务不再有非功能性业务中断困扰的同时，具备轻量、敏捷、高度自动化的特点。云原生架构有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用，其代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API等。

请围绕“论云原生架构及其应用”论题，依次从以下三个方面进行论述：

1.概要叙述你参与管理和开发的软件项目以及承担的主要工作。

2.服务化、弹性、可观测性和自动化是云原生架构的四类设计原则，请简要对这四类设计原则的内涵进行阐述。

3.具体阐述你参与管理和开发的项目是如何采用云原生架构的，并且围绕上述四类设计原则，详细论述在项目设计与实现过程中遇到了哪些实际问题，是如何解决的。

## 云原生架构定义

从技术的角度，云原生架构是基于云原生技术的一组架构原则和设计模式的集合，旨在**将云应用中的非业务代码部分进行最大化的剥离，从而让云设施接管应用中原有的大量非功能特性(如弹性、韧性、安全、可观测性、灰度等),使业务不再有非功能性业务中断困扰的同时，具备轻量、敏捷、高度自动化的特点**。由于云原生是面向“云”而设计的应用，因此，技术部分依赖于传统云计算的3层概念，即基础设施即服务 (IaaS)、 平台即服务 (PaaS) 和软件即服务 (SaaS)。云原生的代码通常包括三部分：业务代码、三方软件、处理非功能特性的代码。其中“业务代码”指实现业务逻辑的代码；“三方软件”是业务代码中依赖的所有三方库，包括业务库和基础库；“处理非功能性的代码”指实现高可用、安全、可观测性等非功能性能力的代码。三部分中只有业务代码是核心，是对业务真正带来价值的，另外两个部分都只算附属物，但是，随着软件规模的增大、业务模块规模变大、部署环境增多、分布式复杂性增强，使得今天的软件构建变得越来越复杂，对开发人员的技能要求也越来越高。云原生架构相比较传统架构进了一大步，从业务代码中剥离大量非功能性特性(不会是所有，比如易用性还不能剥离)到 IaaS和PaaS 中，从而减少业务代码开发人的技术关注范围，通过云厂商的专业性提升应用的非功能性能力。此外，具备云原生架构的应用可以最大程度利用云服务和提升软件交付能力，进一步加快软件开发。



## 素材

### 云原生架构设计原则有七条。

1．服务化原则

通过服务化架构拆分不同生命周期的业务单元，实现业务单元的独立迭代，从而加快整体的迭代速度，保证迭代的稳定性。同时，服务化架构采用的是面向接口编程方式，增加了软件的复用程度，增强了水平扩展的能力。服务化设计原则还强调在架构层面抽象化业务模块之间的关系，从而帮助业务模块实现基于服务流量（而非网络流量）的策略控制和治理，而无须关注这些服务是基于何种编程语言开发的。通过微服务，需要将单体应用进一步拆分，按业务边界重新划分成分布式应用，使应用与应用之间不再直接共享数据，而是通过约定好的契约进行通信，以提高扩展性，业务化垂直扩展（Scale Up），并将微服务数据水平扩展（Scale Out）。

2．弹性原则

系统部署规模可以随着业务量的变化自动调整大小，而无须根据事先的容量规划准备固定的硬件和软件资源。优秀的弹性能力不仅能够改变企业的IT成本模式，使得企业不用再考虑额外的软、硬件资源成本支出（闲置成本），也能更好地支持业务规模的爆发式扩张，不再因为软、硬件资源储备不足而留下遗憾。简言之，弹性原则是指系统部署规模可以随着业务量的变化自动调整大小，而无须根据事先的容量规划准备固定的硬件和软件资源。

3．可观测性原则

强调主动性，在云计算这样的分布式系统中，主动通过日志、链路跟踪和度量等手段，让一次 App 点击所产生的多次服务调用耗时、返回值和参数都清晰可见，甚至可以下钻到每次第三方软件调用、SQL 请求、节点拓扑、网络响应等信息中。运维、开发和业务人员通过这样的观测能力可以实时掌握软件的运行情况，并获得前所未有的关联分析能力，以便不断优化业务的健康度和用户体验。简言之，可观测性更强调主动性，在云计算这样的分布式系统中，主动通过日志、链路跟踪和度量等手段，让一次App 点击所产生的多次服务调用耗时、返回值、参数都可见。

4．韧性原则

韧性是指当软件所依赖的软、硬件组件出现异常时，软件所表现出来的抵御能力。这些异常通常包括硬件故障、硬件资源瓶颈（如CPU或网卡带宽耗尽）、业务流量超出软件设计承受能力、影响机房正常工作的故障或灾难、所依赖软件发生故障等可能造成业务不可用的潜在影响因素。业务上线之后，在运行期的大部分时间里，可能还会遇到各种不确定性输入和不稳定依赖的情况。当这些非正常场景出现时，业务需要尽可能地保证服务质量，满足当前以联网服务为代表的“永远在线”的要求。因此，韧性能力的核心设计理念是面向失败设计，即考虑如何在各种依赖不正常的情况下，减小异常对系统及服务质量的影响并尽快恢复正常。简言之，韧性是指当软件所依赖的软、硬件组件出现异常时，软件所表现出来的抵御能力。韧性原则的实践与常见架构主要包括：服务异步化能力、服务治理能力（重试／限流／降级／熔断／反压）、主从模式、集群模式、多可用区（Availability Zone，AZ）的高可用、单元化、跨区域（Region）容灾、异地多活容灾等。

5．自动化原则

通过IaC、GitOps、OAM、Operator和大量自动化交付工具在CI／CD（持续集成／持续交付）流水线中的实践，企业可以标准化企业内部的软件交付过程，也可以在标准化的基础上实现自动化，即通过配置数据自描述和面向终态的交付过程，实现整个软件交付和运维的自动。

6．零信任原则

传统安全架构认为防火墙内的一切都是安全的，而零信任模型假设防火墙边界已经被攻破，且每个请求都来自于不可信网络，因此每个请求都需要经过验证。第一，不能基于IP配置安全策略；第二，身份应该成为基础设施；第三，假设物理边界被攻破，需要严格控制安全半径。

7．架构持续演进原则

云原生架构本身也应该且必须具备持续演进的能力，而不是一个封闭式的，被设计后一成不变的架构。特别是在业务高速迭代时，更应该考虑如何保证架构演进与业务发展之间的平衡。演进式架构是指软件开发的初始阶段，就可以通过可拓展和松耦合设计，让后续可能发生的变更更加容易，升级性重构的成本更低，并且能够发生在开发实践、发布实践和整体敏捷度等软件生命周期中的任何阶段。

### 云原生架构模式

1.服务化架构模式
服务化架构是云时代构建云原生应用的标准架构模式，要求以应用模块为颗粒度划分一个软件，以接口契约(例如 IDL) 定义彼此业务关系，以标准协议 (HTTP、gRPC等)确保彼此的互联互通，结合DDD (领域模型驱动)、 TDD (测试驱动开发)、容器化部署提升每个接口的代码质量和迭代速度。服务化架构的典型模式是微服务和小服务模式，其中小服务可以看作是一组关系非常密切的服务的组合，这组服务会共享数据，小服务模式通常适用于非常大型的软件系统，避免接口的颗粒度太细而导致过多的调用损耗(特别是服务间调用和数据一致性处理) 和治理复杂度。通过服务化架构，把代码模块关系和部署关系进行分离，每个接口可以部署不同数量的实例，单独扩缩容，从而使得整体的部署更经济。此外，由于在进程级实现了模块的分离，每个接口都可以单独升级，从而提升了整体的迭代效率。但也需要注意，服务拆分导致要维护的模块数量增多，如果缺乏服务的自动化能力和治理能力，会让模块管理和组织技能不匹配，反而导致开发和运维效率的降低。

2. Mesh化架构模式

Mesh化架构是把中间件框架(如 RPC、 缓存、异步消息等)从业务进程中分离，让中间件SDK与业务代码进一步解耦，从而使得中间件升级对业务进程没有影响，甚至迁移到另外一个平台的中间件也对业务透明。分离后在业务进程中只保留很“薄”的 Client部分， Client通常很少变化，只负责与Mesh 进程通信，原来需要在SDK中处理的流量控制、安全等逻辑由Mesh 进程完成。整个架构如图14-1所示。

实施Mesh化架构后，大量分布式架构模式(熔断、限流、降级、重试、反压、隔仓……)都由 Mesh进程完成，即使在业务代码的制品中并没有使用这些三方软件包；同时获得更好的安全性(比如零信任架构能力)、按流量进行动态环境隔离、基于流量做冒烟/回归测试等。

3.Serverless模式
Serverless将“部署”这个动作从运维中“收走”,使开发者不用关心应用运行地点、操作系统、网络配置、 CPU性能等，从架构抽象上看，当业务流量到来/业务事件发生时，云会启动或调度一个已启动的业务进程进行处理，处理完成后云自动会关闭/调度业务进程，等待下一次触发，也就是把应用的整个运行都委托给云。
Serverless并非适用任何类型的应用，因此架构决策者需要关心应用类型是否适合于Serverless运算。如果应用是有状态的，由于Serverless 的调度不会帮助应用做状态同步，因此云在进行调度时可能导致上下文丢失；如果应用是长时间后台运行的密集型计算任务，会无法发挥Serverless 的优势；如果应用涉及频繁的外部I/O (网络或者存储，以及服务间调用),也因为繁重的 I/O负担、时延大而不适合。事件驱动架构图如图14-2所示。 Serverless 非常适合于事件驱动的数据计算任务、计算时间短的请求/响应应用、没有复杂相互调用的长周期任务。

4.存储计算分离模式
分布式环境中的 CAP困难主要是针对有状态应用，因为无状态应用不存在C (一致性)这个维度，因此可以获得很好的A (可用性)和P (分区容错性),因而获得更好的弹性。在云环境中，推荐把各类暂态数据(如session)、 结构化和非结构化持久数据都采用云服务来保存，从而实现存储计算分离。但仍然有一些状态如果保存到远端缓存，会造成交易性能的明显下降比如交易会话数据太大、需要不断根据上下文重新获取等，这时可以考虑通过采用时间日志+ 快照(或检查点)的方式，实现重启后快速增量恢复服务，减少不可用对业务的影响时长。

5.分布式事务模式
微服务模式提倡每个服务使用私有的数据源，而不是像单体这样共享数据源，但往往大颗
粒度的业务需要访问多个微服务，必然带来分布式事务问题，否则数据就会出现不一致。架构师需要根据不同的场景选择合适的分布式事务模式。
(1)传统采用 XA 模式，虽然具备很强的一致性，但是性能差。
(2)基于消息的最终一致性 (BASE) 通常有很高的性能，但是通用性有限。
(3)TCC模式完全由应用层来控制事务，事务隔离性可控，也可以做到比较高效；但是对业务的侵入性非常强，设计开发维护等成本很高。
(4)SAGA模式与TCC 模式的优缺点类似但没有try这个阶段，而是每个正向事务都对应一个补偿事务，也是开发维护成本高。
(5)开源项目 SEATA的AT模式非常高性能且无代码开发工作量，且可以自动执行回滚操作，同时也存在一些使用场景限制。

6.可观测架构
可观测架构包括Logging、Tracing、Metrics三个方面，其中Logging提供多个级别 (verbose/debug/warning/error/fatal) 的详细信息跟踪，由应用开发者主动提供； Tracing提供一个请求从前端到后端的完整调用链路跟踪，对于分布式场景尤其有用；Metrics则提供对系统量化的多维度度量。
架构决策者需要选择合适的、支持可观测的开源框架(比如Open Tracing、Open Telemetry等),并规范上下文的可观测数据规范(例如方法名、用户信息、地理位置、请求参数等),规划这些可观测数据在哪些服务和技术组件中传播，利用口志和 tracing信息中的spanid/traceid, 确保进行分布式链路分析时有足够的信息进行快速关联分析。由于建立可观测性的主要目标是对服务 SLO(Service Level Objective) 进行度量，从而优化SLA, 因此架构设计上需要为各个组件定义清晰的 SLO, 包括并发度、耗时、可用时长、容
量等。

7.事件驱动架构
事件驱动架构 (EDA,Event Driven Architecture) 本质上是一种应用/组件间的集成架构模式。
事件和传统的消息不同，事件具有schema, 所以可以校验event的有效性，同时EDA具备QoS保障机制，也能够对事件处理失败进行响应。事件驱动架构不仅用于(微)服务解耦，还可应用于下面的场景中。
(1)增强服务韧性：由于服务间是异步集成的，也就是下游的任何处理失败甚至宕机都不会被上游感知，自然也就不会对上游带来影响。
(2)CQRS(Command Query Responsibility Segregation): 把对服务状态有影响的命令用事
件来发起，而对服务状态没有影响的查询才使用同步调用的API接口；结合EDA 中的 EventSourcing机制可以用于维护数据变更的一致性，当需要重新构建服务状态时，把EDA 中的事件重新“播放”一遍即可。
(3)数据变化通知：在服务架构下，往往一个服务中的数据发生变化，另外的服务会感兴趣，比如用户订单完成后，积分服务、信用服务等都需要得到事件通知并更新用户积分和信用等级。
(4)构建开放式接口：在EDA 下，事件的提供者并不用关心有哪些订阅者，不像服务调用的场景——数据的产生者需要知道数据的消费者在哪里并调用它，因此保持了接口的开放性。
(5)事件流处理：应用于大量事件流(而非离散事件)的数据分析场景，典型应用是基于Kafka 的日志处理。
基于事件触发的响应：在 IoT时代大量传感器产生的数据，不会像人机交互一样需要等待处理结果的返回，天然适合用EDA来构建数据处理应用。

### 云原生涉及的技术

#### 容器编排

Kubernetes 已经成为容器编排的事实标准，被广泛用于自动部署，扩展和管理容器化应用。
Kubernetes 提供了分布式应用管理的核心能力。
● 资源调度：根据应用请求的资源量CPU、Memory, 或者GPU等设备资源，在集群中选
择合适的节点来运行应用。
● 应用部署与管理：支持应用的自动发布与应用的回滚，以及与应用相关的配置的管理；也可以自动化存储卷的编排，让存储卷与容器应用的生命周期相关联。
● 自动修复： Kubernetes能监测这个集群中所有的宿主机，当宿主机或者OS出现故障，节点健康检查会自动进行应用迁移； K8s也支持应用的自愈，极大简化了运维管理的复杂性。
● 服务发现与负载均衡：通过Service资源出现各种应用服务，结合DNS和多种负载均衡机制，支持容器化应用之间的相互通信。
● 弹性伸缩： K8s可以监测业务上所承担的负载，如果这个业务本身的CPU利用率过高，或者响应时间过长，它可以对这个业务进行自动扩容。Kubernetes 的控制平面包含四个主要的组件： APIServer、Controller、Scheduler 以及etcd。

● 声明式API: 开发者可以关注于应用自身，而非系统执行细节。比如Deployment (无状态应用)、 StatefulSet (有状态应用)、 Job (任务类应用)等不同资源类型，提供了对不同类型工作负载的抽象；对Kubernetes实现而言，基于声明式API的 “level-triggered” 实现比 “edge-triggered” 方式可以提供更加健壮的分布式系统实现。
● 可扩展性架构：所有K8s组件都是基于一致的、开放的API实现和交互；三方开发者也可通过CRD(Custom Resource Definition)/Operator等方法提供领域相关的扩展实现，极大提升了K8s的能力。
● 可移植性： K8s通过一系列抽象如Load Balance Service (负载均衡服务)、 CNI (容器网络接口)、 CSI (容器存储接口),帮助业务应用可以屏蔽底层基础设施的实现差异，实现容器灵活迁移的设计目标。







## 实例

**云原生架构的四类设计原则**

1. **服务化**：通过将应用拆分为多个独立的、可复用的服务，实现服务的松耦合和独立部署。这有助于提高系统的可维护性和可扩展性。
2. **弹性**：系统能够根据负载情况自动调整资源分配，确保在高并发场景下仍能保持稳定的性能。
3. **可观测性**：通过监控和日志收集等手段，实时了解系统的运行状态和性能指标，以便及时发现和解决问题。
4. **自动化**：利用自动化工具和流程，减少人工干预，提高系统的部署、升级和维护效率。

**三、云原生架构在拍卖平台项目中的应用**

1. **服务化改造**：我们将拍卖平台拆分为用户服务、拍卖服务、支付服务等多个微服务。每个服务都独立部署在容器中，并通过服务网格进行通信。这种架构使得我们可以独立地对每个服务进行开发和部署，提高了开发效率。
2. **弹性扩展**：我们利用Kubernetes等容器编排工具，实现了系统的弹性扩展。当系统负载增加时，可以自动增加容器实例数，以提高处理能力；当负载减少时，则可以自动减少容器实例数，以节省资源。
3. **可观测性实现**：我们引入了Prometheus等监控工具，对系统的运行状态和性能指标进行实时监控。同时，我们还使用了ELK（Elasticsearch、Logstash、Kibana）日志收集和分析系统，对系统的日志进行收集、存储和分析。这些工具帮助我们及时发现和解决了系统中的问题。
4. **自动化部署与运维**：我们利用Jenkins等自动化工具，实现了代码的自动构建、测试和部署。同时，我们还引入了Ansible等自动化运维工具，实现了系统的自动化配置和管理。这些工具减少了人工干预，提高了系统的稳定性和可靠性。

**四、结论与反思**

在项目实施过程中，我们遇到了诸多实际问题。例如，在微服务拆分过程中，如何确保服务的独立性和可复用性；在弹性扩展过程中，如何避免资源的浪费和瓶颈；在可观测性实现过程中，如何确保监控数据的准确性和实时性；在自动化部署与运维过程中，如何确保系统的稳定性和安全性等。针对这些问题，我们积极寻求解决方案，并通过实践不断优化和改进。

云原生架构以其轻量、敏捷、高度自动化的特点，为拍卖平台项目带来了诸多优势。通过服务化改造、弹性扩展、可观测性实现和自动化部署与运维等手段，我们成功地将云原生架构应用于项目中，并解决了诸多实际问题。未来，我们将继续探索云原生架构的更多应用场景和最佳实践，为企业的发展提供更有力的支持。





## 我的项目大纲



服务化原则：根据功能将项目拆分为多个功能模块，通过分析模块的共性汇聚成多个微服务，包括权限管理服务、数据分析与处理服务、告警管理服务、资源管理服务等主要服务，对网络进行管理的注册中心服务。通过docker容器进行部署

弹性扩展：在服务的集群化部署过程中，不是根据部署时设置的服务部署数目来固定的设置某个服务的集群数目，而是根据服务的负载来自动化的进行扩容和缩容。项目部署在docker容器中，通过k8s管理工具来对服务进行容器编排，加入KEDA来对负载扩缩容策略进行定义，当负载减小时减少服务数目，当负载增加时自动触发扩容，增加集群的中微服务的部署数目。

可观测性实现：在云计算这样的分布式系统中，主动通过日志、链路跟踪和度量等手段，让一次 App 点击所产生的多次服务调用耗时、返回值和参数都清晰可见，甚至可以下钻到每次第三方软件调用、SQL 请求、节点拓扑、网络响应等信息中。运维、开发和业务人员通过这样的观测能力可以实时掌握软件的运行情况，并获得前所未有的关联分析能力，以便不断优化业务的健康度和用户体验。通过心跳方式对微服务进行心跳基本的运行检测，不同于传统的简单心跳检测，在项目中通过发送心跳检查关键接口的运行情况，执行关键模块的检查脚本来完成软件的运行状态检查。通过对微服务日志进行分级别、分影响程度进行区分、同时对不同异常日志进行标签化处理，通过统一数据结构实现对日志的多维存储，通过对日志进行分析实现对所有微服务进行监控和检测。



自动化原则：通过IaC、GitOps、OAM、Operator和大量自动化交付工具在CI／CD（持续集成／持续交付）流水线中的实践，企业可以标准化企业内部的软件交付过程，也可以在标准化的基础上实现自动化，即通过配置数据自描述和面向终态的交付过程，实现整个软件交付和运维的自动。项目中采用Jekins+GitLab结合方式，对代码的全方位自动化静态检测、动态检测、单元测试、集成测试、自动化部署，自动化交付，金丝雀发布等过程。

本文以我参与公司的"统一网络设备管理平台"项目为例，论述云原生架构设计方法。公司一家通信公司，主营业务是通信设备，该项目的目标完成对公司5G、5GA设备的统一管理，实现对设备进行资源管理、资源数据管理、告警管理、统一配置管理等功能，在此项目中，我作为系统架构师，主导了该项目的管理工作。项目中选择以容器化与微服务化为主要模式的云原生，云原生架构是基于云原生技术的一组架构原则和设计模式的集合，我将在下文中论述项目中对于云原生架构的使用和落地，并结合服务化、弹性扩展、可观测性、自动化原则四种基本原则来叙述在项目设计阶段遇到的问题。通过云原生架构的使用落地，2024年实现了项目的稳定运行和持续优化，获得了领导用户的一致好评。

近年来，随着网络设备的更新迭代，5G设备连通到千家万户，公司的业务也持续发展，业务扩充到海内外，这使得公司对于网络设备的统一管理越发重视。因此，在2023年，公司启动了新一代的网络设备的统一管理软件的研发工作，公司投资2000万，计划8个月分两期完成，前期完成服务软件的设计开发，后期完成软件的自动化流程。我作为本项目的架构师，全面负责对统一网络设备管理平台的分析设计工作，并在项目中实践了云原生架构设计方法，得到了项目组成员和公司高层的认可，下面我将重点阐述我在本项目中关于云原生架构设计方法和实践。

1、利用微服务与容器化技术实现服务的一键自动部署、自动扩容缩容，提高可用性，可靠性

2、结合云原生架构的数据构建管理运营能力，实现多地数据统一网络管理能力，进行网络数据持久化

3、结合DevOps的应用开发模式，实现敏捷开发，提升业务的迭代速度，高效是实现业务需求，保证全流程的安全迭代。

背景：

1、云原生架构是基于云原生技术的一组架构原则和设计模式的集合，旨在将云应用的非业务代码部分进行最大化的剥离，从而让云设施接管引用用原有的大量非功能特性，让业务不再有非功能性业务中断的困扰的同时，具备轻量、敏捷、高度自动化特点。云原生模式包括：1、服务化架构模式。2、Mesh架构模式。3、存储计算分离模式。4、Servlerless模式。5、分布式事务模式。6、可观测性模式.7、事件驱动架构模式。在具体实践中，结合项目的特点和非功能性需求以及云计算平台特性，来综合采用不同的架构模式。来实现项目的性能指标。

2、公司开展迁移上云行动，最大限度的利用云平台弹性、分布式、自助、按需、等产品优势、

降低交付周期、充分利用资源

公司发展：XXX。业务的快速增长让公司的团队感到欣喜，但另一方面需要直面高流量带来的挑战，云原生改造成为解决问题的关键。

挑战：

云原生解决方案

1、服务器统一回收，旧有系统回收改造、收拢一套以云原生为核心的私有平台。同时将将 IDC、 物理网络、虚拟网络、计算资源、存储资源等通过laaS、PaaS 等，实现虚拟化封装、切割、再投产的自动化流程。

  	首先，公司基于之前的服务器机房搭建一整套服务器环境，为云原生提供资源基础，对就有系统进行回收改造，收拢一套以云原生为核心的私有平台，在之前的操作系统基础上。将IDC、物理网络、私有网络、存储资源等进行虚拟化封装，为项目提供IaaS、PaaS等资源，为资源再投产提供物理环境基础。在此基础上，搭建一套基于K8s的集群化管理工具，实现对容器的编排，同时基于K8s的ApiServer，实现对容器资源、网络资源、存储资源等可视化监控。

2、基础设施容器化改造

​	项目环境中涉及众多的服务组件，在涉及之初需要对服务组件进行梳理。在数据存储层面，包括关系型数据库、非关系型数据库、索引数据库。对于网络传递层面，包括网络服务（Nginx和Integress），实现事件驱动的消息传递的中间件，服务注册与发现中间件等。对项目所涉及的的软件进行分类后进行确定持久化方案，同时作为基础设施服务为开发成员提供基本的基础环境，使开发人员专注于业务功能的实现。

​	在基础设施服务搭建过程中，利用K8s的方式来对基础设施进行一键部署，既能使开发人员按需部署，按需取用。因此开发一套基于K8S的基础设施平台，在存储上通过PV、PVC机制完成不同类型数据库的持久化资源分配。在网络层面，设定内部与外部网络的不同路径，针对外部，建立以Nginx+Integress为主的网络资源分配方式，在内部，结合Serveic服务与服务注册中心，提供多层次，多维度的网络服务，同时网络支持扩容缩容，能够是实现内部，外部的网络负载均衡。

​	在上述网络和存储资源基础化的基础上，由于统一硬件管理平台的协议复杂性，例如支持基于Dubbo的RPC服务访问方式、支持Rest为主要形式的接口访问方式，基于SNAMP协议为主要硬件直接读取的访问方式，还有部分硬件设备是通过C语言完成的三次握手四次挥手自写的仿Http协议连接方式，业务协议极其复杂。基于此，考虑到需要对多种协议进行兼容支持，实现对协议进行协议上云管理，通过整合多种协议改造为一个基础设施，实现对多种协议进行融合处理，完成此项改造。

3、应用微服务化

​	在完成硬件基础设施和平台基础设施的搭建的基础上，对项目应用进行拆分，微服务应用由于能独立部署、独立开发、满足单一职责原则，而且有一整套的微服务云原生的解决方案，K8s的容器化编排技术天生适应微服务化的应用实现。因此，我们决定采用微服务与docker容器技术方式进行服务部署。用K8s进行微服务容器进行编排。我们根据需求对项目的功能职责进行梳理，功能点进行归类，划分微服务应用的边界，拆分出几个基本的微服务软件，包括权限管理服务、资源管理服务、数据处理与计算服务、资源配置与告警服务。在统一的RESET/HTTP协议上定义了各自的API，输出每个应用的docker镜像，做了自动化部署平台。	在拆分多个服务后，实现了业务的分布式部署，然而在分布式的复杂性，需要利用适应分布式理论支撑才能保证软件的可用性，分布式事务、分布式锁、分布式缓存等都需要在架构层面进行统一涉及。在分布式事务中，我们采用TCC理论，TCC虽然需要进行代码侵入式处理，但确实是解决分布式事务性能最高的方式，由于项目管理海量的设备，对于性能要求较高，因此采用此种方式，团队之间通过约定Try-Confirm-Cancel的情况，通过业务代码来实现分布式事务，同时要求团队各成员对分布式事务涉及的微服务进行统一代码走查与结对编程，更好减少问题出现情况。对于分布式缓存、分布式锁通过redis作为缓存中间件，redis是一种内存数据库，因此其性能较高，通过设计redis的集群模式-哨兵模式实现更高性能的缓存支持，同时支持分布式缓存与分布式锁。

​	除了基本微服务化部署外，自动化扩容缩容是与原生微服务的显著特点之一，通过自动扩容和缩容一方面能够方便的应对业务数据流量在不同时资源性能不会衰减，另一方面也能充分利用硬件资源，避免资源浪费。在设备统一管理平台项目中业务存在时间流量洪峰的特点，例如由于白天人的活动量更大一些，用手机的时间更长，相应的打电话的时间也更长，此时产生的话务数据会更加庞大，然而无法具体确定什么时间点数据量较大，利用K8s的自动扩缩容机制能够方面的实现当流量大时增大性能数据处理服务的节点数目，通过这种增大集群进行节点冗余方式完成流量的分发，降低单节点压力，当然，不仅仅的应用服务，在网络基础服务、存储资源服务同样适用于此流程。

4、自动化部署测试与交付

​	云原生架构通过IaC、GitOps、OAM、Operator和大量自动化交付工具在CI／CD（持续集成／持续交付）流水线中的实践，企业可以标准化企业内部的软件交付过程，也可以在标准化的基础上实现自动化，即通过配置数据自描述和面向终态的交付过程，实现整个软件交付和运维的自动。项目中为了实现应用的快速迭代，打通从项目代码编写-单元测试-部署-集成测试-交付等流程，为团队敏捷开发提供支持，实现两周一迭代目标，减少开发与测试之间的沟通障碍，基于云原生架构建立起一套自动化部署测试与交付平台。项目中采用Jekins+GitLab结合方式，建立起一套流水线。首先，开发编写代码与测试用用例，提交代码到GitLab，触发Jeking进行流水线扫描流程，主要流程包括，执行单元测试，调用圈复杂度扫描工具进行静态检查，公司购置一套kw、Convertity工具进行动态漏洞扫描，扫描完成后如果正常则提交给开发专家审核，不通过则提交失败，并生成失败报告提供给开发者检查修改。开发专家审核通过后会自动合并到代码，代码进行编译部署到服务器，触发集成测试，包括基于RobotFramework的接口测试和基于Selenimu的模拟浏览器测试，并生成报告给测试人员，测试人员将会进一步的测试，有问题则提交漏洞给项目管理人员，项目管理人员分配给具体开发人员，通过这种迭代方式进行快速的代码提交和验证，最终每天生成一个中间版本，根据开发计划生成Release版本，进行自动化部署发布。同时在发布工程中支持灰度发布，金丝雀发布，在云山环境完成自动化的从开发到测试，最后进行版本发布的全流程管理。

​	通过建立起对项目支持的自动化流水线流程，项目的施行敏捷开发模式，进行版本快速迭代，同时实现DevOps的开发方式，集开发运维一体化，对软件问题快速处理，极大的提高项目的进度，也为后续的软件维护打下坚实基础。

5、可视化观测实现软件全套管理

​	通过云原生架构对分布式的微服务应用进行监控是极为复杂的，云原生架构中为实现应用的可观测性，需要定义不同日志级别对不同应用的链路和异常进行追踪。在设计项目的架构时发现，应用主要存在两种，一种的功能性应用，另一种时非功能性应用。功能性应用主要通过logback+Elasticsearch来是实现。Elasticsearch是一种以json格式存储的非关系文档数据库。功能性应用是软件开发者可控的服务，开发者通过设置日志级别存入不同的索引中，完成单个微服务的日志的记录；同时，对于关键业务的链路，指定不同的索引实现链路的追踪，运维人员通过检查索引日志来快速定位问题。而对于非功能性应用，包括Mysql、redis、kafka等基础设施中间件，由于日志方式无法自定义，因此通过ELK（Elasticsearch+LogStash+Kibnna）对不同节点产生的日志进行采集及统一处理，实现对基础设施中间件的日志追踪。通过上述两种日志采集方式实现整体架构的日志管理，并基于此，通过对日志的监控实现应用的可观测性。在落地过程中，由于对不同中间件的理解不够深入，在实现过程中对于错误日志的产生情况和监控效果不佳，通过检查官方文档和收集资料对不同中间件的日志进行统一的整理，通过匹配法对日志进行过滤，完成对基础设施的软件监控。

​	通过对项目的设计与分析，拆分四个小组，包括基于K8s的的基础设施扩展团队，基于统一设备管理平台的核心应用开发团队，基于自动化运维与测试的测试团队，三个小组通力合作，最终与2024年4月1日成功交付项目，并于5月1日进行了项目的平稳落地，目前已经肩负起多个局点和政企平台的设备管理，并进一步的兼容更多的设备，获得了客户、领导的一致好评。我的团队也在本次项目上云工作中收获颇丰。









