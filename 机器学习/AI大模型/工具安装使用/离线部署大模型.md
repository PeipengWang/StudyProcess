默认情况下ollama模型的所在路径为：

```bash
macOS: ~/.ollama/models 
Linux: **/usr/share/ollama/.ollama/models**
Windows: C:Users<username>.ollamamodel
```

自己在服务器上创建一个路径，打开ollama配置文件/etc/systemd/system/ollama.service

在里面加入你新创建的路径

#### 1、大模型源文件准备

（1）国内大模型源文件下载地址https://modelscope.cn/models

（2）国外大模型源文件下载地址https://huggingface.co/models

#### 2、准备Modelfile文件

（1）Modelfile参数说明

| 命令                 | 描述                               |
| -------------------- | ---------------------------------- |
| FROM (必需的)        | 引入使用的模型或者模型源文件       |
| PARAMETER(参数)      | 设置大模型的运行参数               |
| TEMPLATE(提示词模板) | 用于大模型请求的 prompt 提示词模板 |
| SYSTEM               | 设置的大模型默认的系统消息         |
| ADAPTER              | 定义适用于模型的（Q）LoRA 适配器   |
| LICENSE              | 指定 license.                      |
| MESSAGE              | 指定消息历史                       |

（2）Modelfile样例文件

```
FROM ./codeqwen-1_5-7b-chat-q8_0.gguf

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ .Response }}<|im_end|>
"""
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
```

#### 3、创建模型

（1）创建模型命令

```
# 创建存储大模型源文件和modelfile文件的目录
sudo mkdir /home/llm
# 上传文件codeqwen-1_5-7b-chat-q8_0.gguf和modelfile-codeqwen.Modelfile到/home/llm；
# 进入工作目录
cd /home/llm
# 执行创建命令
ollama create codeqwen:7b -f ./modelfile-codeqwen.Modelfil
# 查看创建结果
ollama list
```

![在这里插入图片描述](https://raw.githubusercontent.com/PeipengWang/picture/master/exama7657867359abf0fd4b9af8fb606e681.png)

#### 4、运行模型

（1）运行模型命令

```
ollama run <your-model-name>
```

#### 5、删除模型

（1）删除模型命令

```
ollama rm <your-model-name>
```

2）删除模型示例

```bash
# 删除步骤3创建的codeqwen:7b大模型
ollama rm codeqwen:7b
```