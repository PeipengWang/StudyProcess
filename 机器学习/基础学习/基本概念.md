## 偏置与权重



## 激活函数

## 损失函数

### **损失函数是什么**

损失函数（Loss Function）是机器学习和深度学习模型训练过程中用来评估模型预测结果与真实值之间差异的函数。它是一个标量函数，将模型的预测结果与目标值映射到一个非负实数，用于量化预测的“错误程度”。

### **损失函数的作用**

1. **衡量模型的性能：**
   - 损失函数的值（即损失值）是模型预测的准确性的直接反映。损失值越小，说明模型预测结果越接近真实值。
2. **指导模型参数的更新：**
   - 在训练过程中，模型通过优化算法（如梯度下降）最小化损失函数。损失函数为模型的优化提供了目标方向。
   - **目标：**通过不断调整模型参数，使损失函数的值尽可能小。
3. **避免过拟合或欠拟合：**
   - 适当选择损失函数可以帮助模型更好地学习数据的特征，而不会过分拟合训练集或对数据特征学习不足。

## 梯度法

梯度法是机器学习和深度学习优化的核心工具，通过计算目标函数的梯度引导模型参数逐步逼近最优解。理解梯度法不仅有助于优化算法的实现，还能帮助更好地调试和改进模型的训练过程。

## **Mini-Batch学习简介**

Mini-Batch学习是机器学习中一种介于**批量梯度下降（Batch Gradient Descent, GD）**和**随机梯度下降（Stochastic Gradient Descent, SGD）**之间的优化方法。它通过将训练数据划分为多个小批量（Mini-Batch），每次迭代只用一个小批量数据来计算梯度和更新模型参数。

------

### **Mini-Batch学习的特点**

1. **小批量数据：**
   - 将整个数据集拆分为多个小批量，每个小批量的数据量称为**Batch Size**。
   - 例如，若训练集有 10,000 条数据，Batch Size 为 100，则共有 10,000/100=10010,000 / 100 = 10010,000/100=100 个批次。
2. **平衡效率和稳定性：**
   - 相较于 GD，Mini-Batch减少了每次计算的时间开销。
   - 相较于 SGD，Mini-Batch的更新过程更加平滑，减少了由于噪声引起的梯度振荡。

### **Mini-Batch梯度下降的工作流程**

1. 随机打乱数据集（Shuffle），确保每次训练的小批量数据具有代表性。
2. 将数据分成多个小批量（Mini-Batch）。
3. 对每个小批量：
   - 计算损失函数的梯度。
   - 使用梯度更新模型参数。
4. 重复上述过程，直到达到预定的迭代次数或损失函数收敛。

### **Mini-Batch学习的优点**

1. **计算效率高：**
   - Mini-Batch比GD快，因为每次只用一部分数据计算梯度，而不是整个数据集。
2. **更新更加稳定：**
   - 相比于SGD，梯度波动较小，更新方向更加稳定。
3. **硬件优化：**
   - Mini-Batch可以更好地利用向量化和GPU并行计算，从而提高计算效率。
4. **适合大规模数据集：**
   - 当数据量较大时，GD对整个数据集求梯度非常耗时，而Mini-Batch可以有效地降低计算开销。

### **Mini-Batch学习的缺点**

1. **选择Batch Size有难度：**
   - 太小的Batch Size可能导致训练过程不稳定，容易陷入局部最优解。
   - 太大的Batch Size则可能使模型更新慢，并需要更多内存。
2. **梯度估计可能不够精确：**
   - 由于Mini-Batch仅使用部分数据，梯度估计可能与整个数据集的真实梯度存在偏差。

### **Mini-Batch学习的超参数**

1. **Batch Size的选择：**
   - 通常为 32、64、128 或 256 等2的幂次。
   - 小型数据集：较大Batch Size（例如 256）。
   - 大型数据集：较小Batch Size（例如 32 或 64）。
   - 需要根据具体问题和硬件资源调试。
2. **学习率（Learning Rate, η\etaη）：**
   - 较小Batch Size时，适当减小学习率以避免梯度震荡。