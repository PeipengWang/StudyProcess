# 凤凰架构

## RPC

RPC框架各种变体要解决的三个问题：
如何表示数据：序列化协议
如何传递数据：通过网络在两个服务之间交换数据
如何确定方法：后面通过接口描述语言来实现跨语言平台的方法表示
RPC发展历程
CROAB->Web Service->分裂阶段
RMI
Thrift
Dubbo
gRPC
Motan
Fingle
brpc
基本上在三个方向发展：
面向对象
性能优先：gRPC，Thrift
简化发展：JSON-RPC

可配置式Dubbo
Dubbo
Json需求则加fastjson，追求更高的性能可替换为Kryo，FST，Potocol Buffers等效率更高的序列化器

## Rest

一套理想的Rest架构满足六大原则：
1、前后端分离（服务端与客户端分离）
2、无状态
3、可缓冲
4、分层系统
5、统一接口
6、按需代码
优点：
学习成本低，把资源放到标准的Http方法就好
资源天然具有集合与层次结构
REST绑定于HTTP协议
缺点和争议：
面向资源的编程思想只适合做 CRUD，面向过程、面向对象编程才能处理真正复杂的业务逻辑，这是遇到最多的一个问题。
REST 与 HTTP 完全绑定，不适合应用于要求高性能传输的场景中
REST 不利于事务支持
REST 没有传输可靠性支持
REST 缺乏对资源进行“部分”和“批量”的处理能力

**一种理论上较优秀的可以解决以上这几类问题的方案是GraphQL**

## 事务处理

### 本地事务：单服务单数据源

### 全局事务：单服务多数据源

XA处理事务架构：定义了全局的事务管理器和局部的资源管理器之间的通信接口
是一种解决多数据源事务提交过程中某个失败最后只有部分数据源提交成功问题
通过分阶段来解决问题：

* 准备阶段：协调者询问所有参与者是否做好准备
* 提交阶段：协调者根据上一阶段收到所有参与者回复的Prepared消息，先在自己本地持久化事务状态Commit，在此操作完成后向所有参与者发送立即提交的操作，否则任一参与者回复了Non-Prepared消息则协调者将自己事务持久化为Abort后，向消息参与者发送Abrot指令。参与者回滚

这个过程应该尽可能短，还没有提交还能回退，如果提交了那么没法回退了。
一些节点因为网络分区或者崩溃必须快速恢复，并询问协调者查询事务状态，确定下一步是提交还是回滚。

XA的两阶段提交（2PC）会出现的问题：

1. 单点问题：协调者只有一个，宕机所有参与者只能等待
2. 性能问题：多个参与者只有都回复了，协调者才会响应，造成受到时间最长的参与者的限制
3. 一致性风险：多个参与者在得到参与者响应的阶段也不一定能执行成功，造成部分最终提交，部分为成功

为了解决上述单点问题提出了三阶段提交
准备阶段：CanCommit、PreCommit
提交阶段：DoCOmmit
1、引入超时机制。同时在协调者和参与者中都引入超时机制。
2、在第一阶段和第二阶段中插入一个准备阶段，保证了在最后提交阶段之前各参与节点状态的一致。


注意这两个阶段的不同点：
CanCommit阶段--这一阶段世界上是为了防止写入redolog和undolog而提前准备的
在这个阶段，事务协调者询问所有的参与者是否可以提交事务。
参与者在这个阶段仅回答事务是否可以提交，**而不做任何实际的提交动作。**（不会去写入redolog 和undolog)
这个阶段的目的是为了获取所有参与者的同意，以便在后续阶段做出进一步的决定。

PreCommit阶段--这里才开始正式的写日志
在这个阶段，如果所有的参与者都同意提交事务（CanCommit阶段返回"Yes"），事务协调者向所有的参与者发出预提交请求。
参与者在收到预提交请求后，执行实际的事务提交操作，并返回"完成"或"失败"的结果。
这个阶段的目的是实际执行事务的提交操作，但是这个阶段的提交是有条件的，只有在所有的参与者都预提交成功的情况下，才会继续到下一个阶段。

还是会有两个问题：
三阶段提交实际上只是增加了在回滚情况下的效率，但是对于正常情况下反而多加了一个询问，是有负面影响的。
一致性风险没有任何改进，超时提交事务机制也会引起不一致问题

### 共享事务：多服务单数据源

方案一：各个服务共享数据库连接
如WebSphere
方案二：新增一个中间角色
每个服务传给这个中间角色同一事务id来完成事务提交。
但是通常情况下，数据库才是瓶颈，因此一般不会这样用

但是有种变体形式：用消息队列类代替这个中间角色

### 分布式事务：多服务多数据源

CAP与ACID的矛盾性
CAP概念与放弃会产生的问题
P:分区容忍性，网络都会有延迟，同行也不一定可靠，这个是不可能放弃的
A（CP系统）：可用性，如果放弃实际上可能会退化为一种全局性事务，可以通过2PC/3PC的方式来获得分区容忍性和一致性
C（AP系统）：一致性，网络分区的时候，节点之间的数据天然不一致， A(可用性)是建设分布式架构的目的，一般不会放弃，除非接受为了强一致性放弃可用性，如银行业务。

选择放弃一致性的 AP 系统目前是设计分布式系统的主流选择，事务的目的是为了获得一致性，但是在分布式系统中不得不为了性能放弃一部分一致性。在放弃强一致性情况下获得最终一致性称为主要设计的目的。

解决方案：
可靠消息事件：靠着持续重试来保证可靠性的解决方案，没有失败回滚的概念，必须成功，失败则重发消息
                     被称为最大努力交付（Best Effort Deliver)
上述方案缺少隔离性，例如：导致超卖问题
TCC事务：
Try：尝试执行阶段，完成所有业务的可执行检查，并且预留号所需的全部资源
Confirm：确认阶段，不进行业务检查，利用Try阶段准备的资源完成业务处理，COnfirm阶段可能会重复执行，因此本阶段需要幂等性
Cancle：取消执行阶段，释放Try阶段预留的业务资源

实际上是检查每个服务的阶段能不能完成服务，能完成则冻结服务，所有服务都能执行才会进行Confirm执行执行资源修改，否则进行cancle来释放资源
Tcc类似2PC，但是2PC位于用户代码层面
典型中间件：seta

TCC缺点：业务侵入性很强
SAGA事务：
通过补偿的方式，对每一个阶段的事务提交进行补偿，根据数据日志产生用于补偿的“逆向SQL”来对事务进行回滚
然而保证其隔离性也称为了一个比较麻烦的事，譬如，当本地事务提交之后、分布式事务完成之前，该数据被补偿之前又被其他操作修改过，即出现了脏写（Dirty Write），这时候一旦出现分布式事务需要回滚，就不可能再通过自动的逆向 SQL 来实现补偿，只能由人工介入处理了。

Seata的SAGA模式和AT模式


## 透明多级分流系统

客户端缓存：Http规定的浏览器本地缓存
域名解析：去DNS服务器获取真实IP
传输链路：Http1.1  Http2  http3

CDN的三个阶段：
路由解析
内容分发
CDN应用

负载均衡
数据链路层负载均衡：优势（效率高）和劣势（不能跨子网）共同决定了数据链路层负载均衡最适合用来做数据中心的第一级均衡设备，用来连接其他的下级负载均衡器。
网络层负载均衡： NAT 模式
应用层负载均衡：分为“正向代理”、“反向代理”和“透明代理”三类

几种均衡策略
轮询均衡：从1到N轮流
权重轮询均衡：设置权重后轮流访问
随机均衡
权重随机均衡
一致hash均衡：根据请求的某一数据，可以使MAC，IP或者其它参数作为特征值来落到不同的节点上
响应速度均衡：给各个节点法ping，哪个速度快就用哪个
最少连接数均衡：记录连接数来选择最小的连接数，适合连接时间比较长的，如ftp传输
。。。。。。

也分为硬件均衡器和软件均衡器
硬件均衡器：LVS
软件均衡器：Nginx，HAProxy，KeepAlived等



服务端缓存：
缓存会提高复杂度，需要考虑失效、更新、一致性等问题
引入缓存的目的：缓解CPU压力，缓解IO压力

代码层面的缓存
吞吐量对比：它们在 8 线程、75%读操作、25%写操作下的吞吐量
有 Caffeine、ConcurrentLinkedHashMap、LinkedHashMap、Guava Cache、Ehcache 和 Infinispan Embedded
![](vx_images/361223211288478.png =569x)


命中率与淘汰策略
有限的物理空间决定了缓存必须能够在小号空间和节约时间之间取得平衡，这要求缓存必须能够自动或者由人工淘汰缓存中的低价值数据。

需要明确
什么是低价值数据
这个规定与具体的业务逻辑是无关的，只能从缓存工作过程收集到的统计结果来确定数据是否有价值，通用的统计结果包括但不限于数据何时进入缓存、被使用过多少次、最近什么时候被使用等等，由此决定一旦确定选择何种统计数据，及如何通用的、自动的判定缓存中每个数据价值高低，也相当于决定了缓存淘汰策略是如何实现的，目前淘汰策略有如下几种：
1、FIFO 优先淘汰最早进入被缓存数据，优先级队列，应用场景相对少一些
2、LRU 优先淘汰最晚没被使用的数据， 一些阶段性热点数据可能会被淘汰
3、LFU：优先淘汰最不常被使用的数据，已经不在使用的“旧热点数据”可能很长时间才能被淘汰掉，需要维护一个计数器


LFU变体：
1、TinyLFY
为了缓解计数器性能负担，采用Sketch对访问数据进行统计，借助Count-Min Sketch算法，可以用相对小得多的记录频率和空间近似的找出缓存中的低价值数据。
为了解决热度数据变化的问题：采用滑动时间窗口的热度衰减算法，每隔一段时间，计数器数值会减半

2、W-TinyLFU
实际上是TinyLFU的改进版本，TinyLFU无法很好的应对稀疏突发访问的问题，因此结合LRU特点
在新纪录中暂时放入一个WindowCache的前端LRU缓存里面，让这些对象在WindowCache中积累热度，能通过TinyLRU过滤器则进入MainCache的主缓存过滤器，实际通过分段的方式结合LRU与LFU的优点来实现缓存的策略，每一段是LRU，但总体是LFU


分布式缓存


复制缓存
集中缓存：Redis

进程内缓存与分布式缓存可以互相配合，构成透明多级缓存
![](vx_images/572660598846001.png =364x)

多级缓存结合了进程内缓存和分布式缓存的优点，但是大妈侵入性比较大，需要由开发者承担多次查询、多次回填的工作，也不便于管理，如超时、刷新等策略都要设置多遍，数据更新很麻烦，很容易出现一级缓存和二级缓存互不一致的问题。
因此必须实现“透明”的解决以上问题，多级缓存才有实用价值。
一种常见的做法是以分布式缓存的数据为准，访问以进程内缓存的数据优先
大致做法是当数据发生变动时，在集群内发送推送通知（简单点的话可采用Redis的PUB/SUB，求严谨的话引入ZooKeper或者Etcd来处理），让各个节点的一级缓存自动失效掉相应的数据。

## 架构安全

### 认证：你是谁

安全框架：Apache Shiro和Spring Security

### 授权：你能干什么

常用的多方授权协议主要有 OAuth2 和 SAML 2.0
解决越权问题

### 凭证：你如何证明

Cookie-Session
JWT方案


### 保密

### 传输

### 验证





























