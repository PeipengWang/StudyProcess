# 资源调度

## 1、Label与Selector

Lable+Selector来决定了pod的副本、回滚得策略

### 1.1 label创建方式

Pod加上的标签

- 在各类资源的 metadata.labels 中进行配置\

- 创建临时Label：kubectl label po <资源名称> app=hello

- 修改已经存在的标签：kubectl label po <资源名称> app=hello2 --overwrite

- 查看Lable：

- ```
  # selector 按照 label 单值查找节点
  kubectl get po -A -l app=hello
  
  # 查看所有节点的 labels
  kubectl get po --show-labels
  ```

### 1.2  Selector的创建

利用Select进行查询Label

- 在各对象的配置 spec.selector 或其他可以写 selector 的属性中编写

- kubectl语句创建

 ```
  # 匹配单个值，查找 app=hello 的 pod
  kubectl get po -A -l app=hello
  
  # 匹配多个值
  kubectl get po -A -l 'k8s-app in (metrics-server, kubernetes-dashboard)'
  或 
  
  # 查找 version!=1 and app=nginx 的 pod 信息
  kubectl get po -l version!=1,app=nginx
  
  # 不等值 + 语句
  kubectl get po -A -l version!=1,'app in (busybox, nginx)'
 ```

  

## 2. Deployment

配置文件

```
apiVersion: apps/v1 # deployment api 版本
kind: Deployment # 资源类型为 deployment
metadata: # 元信息
  labels: # 标签
    app: nginx-deploy # 具体的 key: value 配置形式
  name: nginx-deploy # deployment 的名字
  namespace: default # 所在的命名空间
spec:
  replicas: 1 # 期望副本数
  revisionHistoryLimit: 10 # 进行滚动更新后，保留的历史版本数
  selector: # 选择器，用于找到匹配的 RS
    matchLabels: # 按照标签匹配
      app: nginx-deploy # 匹配的标签key/value
  strategy: # 更新策略
    rollingUpdate: # 滚动更新配置
      maxSurge: 25% # 进行滚动更新时，更新的个数最多可以超过期望副本数的个数/比例
      maxUnavailable: 25% # 进行滚动更新时，最大不可用比例更新比例，表示在所有副本数中，最多可以有多少个不更新成功
    type: RollingUpdate # 更新类型，采用滚动更新
  template: # pod 模板
    metadata: # pod 的元信息
      labels: # pod 的标签
        app: nginx-deploy
    spec: # pod 期望信息
      containers: # pod 的容器
      - image: nginx:1.7.9 # 镜像
        imagePullPolicy: IfNotPresent # 拉取策略
        name: nginx # 容器名称
      restartPolicy: Always # 重启策略
      terminationGracePeriodSeconds: 30 # 删除操作最多宽限多长时间

```



### 2.1 功能

#### 2.1.1 创建

```
创建一个 deployment
kubectl create deploy nginx-deploy --image=nginx:1.7.9

或执行
kubectl create -f xxx.yaml --record
--record 会在 annotation 中记录当前命令创建或升级了资源，后续可以查看做过哪些变动操作。

查看部署信息
kubectl get deployments

查看 rs
kubectl get rs

查看 pod 以及展示标签，可以看到是关联的那个 rs
kubectl get pods --show-labels
```

#### 2.1.2 获得deploy的yaml

```
先直接拉取部署
kubectl create deploy nginx-deploy --image=nginx:1.17.0
获取部署的pod
kubectl get pod;
将部署的pod生成yaml文件
kubectl get deploy nginx-deploy -o yaml
```

#### 2.1.3  滚动更新

```
只有修改了 deployment 配置文件中的 template 中的属性后，才会触发更新操作
修改 nginx 版本号
kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1

或者通过 kubectl edit deployment/nginx-deployment 进行修改

查看滚动更新的过程
kubectl rollout status deploy <deployment_name>

查看部署描述，最后展示发生的事件列表也可以看到滚动更新过程
kubectl describe deploy <deployment_name>

通过 kubectl get deployments 获取部署信息，UP-TO-DATE 表示已经有多少副本达到了配置中要求的数目

通过 kubectl get rs 可以看到增加了一个新的 rs

通过 kubectl get pods 可以看到所有 pod 关联的 rs 变成了新
```

**多行滚动更新**

假设当前有 5 个 nginx:1.7.9 版本，你想将版本更新为 1.9.1，当更新成功第三个以后，你马上又将期望更新的版本改为 1.9.2，那么此时会立马删除之前的三个，并且立马开启更新 1.9.2 的任务



**更新配置资源**

![image-20240421171028735](https://raw.githubusercontent.com/PeipengWang/picture/master/image-20240421171028735.png)

**滚动更新过程是先复制一份RS进行升级，然后删除原来的。**

红色是原来的，蓝色是创建的，蓝色创建完成后RS1里面的会逐个删除

![image-20240421172652511](https://raw.githubusercontent.com/PeipengWang/picture/master/image-20240421172652511.png)



#### 2.1.4 回滚

有时候你可能想回退一个Deployment，例如，当Deployment不稳定时，比如一直crash looping。

默认情况下，kubernetes会在系统中保存前两次的Deployment的rollout历史记录，以便你可以随时会退（你可以修改revision history limit来更改保存的revision数）。
案例：
更新 deployment 时参数不小心写错，如 nginx:1.9.1 写成了 nginx:1.91

```
kubectl set image deployment/nginx-deploy nginx=nginx:1.91
```

监控滚动升级状态，由于镜像名称错误，下载镜像失败，因此更新过程会卡住

```
kubectl rollout status deployments nginx-deploy
```

结束监听后，获取 rs 信息，我们可以看到新增的 rs 副本数是 2 个

```
kubectl get rs
```

通过 kubectl get pods 获取 pods 信息，我们可以看到关联到新的 rs 的 pod，状态处于 ImagePullBackOff 状态

为了修复这个问题，我们需要找到需要回退的 revision 进行回退
通过 `kubectl rollout history deployment/nginx-deploy` 可以获取 revison 的列表

通过 `kubectl rollout history deployment/nginx-deploy --revision=2` 可以查看详细信息

确认要回退的版本后，可以通过 `kubectl rollout undo deployment/nginx-deploy` 可以回退到上一个版本

也可以回退到指定的 revision

```
kubectl rollout undo deployment/nginx-deploy --to-revision=2
```

再次通过 `kubectl get deployment` 和 `kubectl describe deployment` 可以看到，我们的版本已经回退到对应的 revison 上了

可以通过设置 .spec.revisonHistoryLimit 来指定 deployment 保留多少 revison，如果设置为 0，则不允许 deployment 回退了。



#### 2.1.5 扩容和缩容

- 通过 kube scale 命令可以进行自动扩容/缩容

- 通过 kube edit 编辑 replcas 也可以实现扩容/缩容，修改replicas值

扩容与缩容只是直接创建副本数，没有更新 pod template 因此不会创建新的 rs

#### 2.1.6 暂停与恢复

由于每次对 pod template 中的信息发生修改后，都会触发更新 deployment 操作，那么此时如果频繁修改信息，就会产生多次更新，而实际上只需要执行最后一次更新即可，当出现此类情况时我们就可以暂停 deployment 的 rollout

通过 `kubectl rollout pause deployment <name>` 就可以实现暂停，直到你下次恢复后才会继续进行滚动更新

尝试对容器进行修改，然后查看是否发生更新操作了

```
kubectl set image deploy <name> nginx=nginx:1.17.9
kubectl get po 
```

通过以上操作可以看到实际并没有发生修改，此时我们再次进行修改一些属性，如限制 nginx 容器的最大cpu为 0.2 核，最大内存为 128M，最小内存为 64M，最小 cpu 为 0.1 核

```
kubectl set resources deploy <deploy_name> -c <container_name> --limits=cpu=200m,memory=128Mi --requests=cpu100m,memory=64Mi
```

通过格式化输出 `kubectl get deploy <name> -oyaml`，可以看到配置确实发生了修改，再通过 kubectl get po 可以看到 pod 没有被更新

那么此时我们再恢复 rollout，通过命令 `kubectl rollout deploy <name>`

恢复后，我们再次查看 rs 和 po 信息，我们可以看到就开始进行滚动更新操作了

```
kubectl get rs
kubectl get po
```

### 2.2 配置文件

```
apiVersion: apps/v1 # deployment api 版本
kind: Deployment # 资源类型为 deployment
metadata: # 元信息
  labels: # 标签
    app: nginx-deploy # 具体的 key: value 配置形式
  name: nginx-deploy # deployment 的名字
  namespace: default # 所在的命名空间
spec:
  replicas: 1 # 期望副本数
  revisionHistoryLimit: 10 # 进行滚动更新后，保留的历史版本数
  selector: # 选择器，用于找到匹配的 RS
    matchLabels: # 按照标签匹配
      app: nginx-deploy # 匹配的标签key/value
  strategy: # 更新策略
    rollingUpdate: # 滚动更新配置
      maxSurge: 25% # 进行滚动更新时，更新的个数最多可以超过期望副本数的个数/比例
      maxUnavailable: 25% # 进行滚动更新时，最大不可用比例更新比例，表示在所有副本数中，最多可以有多少个不更新成功
    type: RollingUpdate # 更新类型，采用滚动更新
  template: # pod 模板
    metadata: # pod 的元信息
      labels: # pod 的标签
        app: nginx-deploy
    spec: # pod 期望信息
      containers: # pod 的容器
      - image: nginx:1.7.9 # 镜像
        imagePullPolicy: IfNotPresent # 拉取策略
        name: nginx # 容器名称
      restartPolicy: Always # 重启策略
      terminationGracePeriodSeconds: 30 # 删除操作最多宽限多长时间

```

## 3、 StatefulSet

### 3.1 配置文件

```
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet # 资源类型
metadata:
  name: web  # StatefulSet对象的名字
spec:
  serviceName: "nginx"  # 使用哪个service来管理dns
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80  # 具体暴露在容器内部的端口
          name: web   #该端口配置的名字
        volumeMounts:  #加载数据卷
        - name: www # 指定挂载的数据卷名称
          mountPath: /usr/share/nginx/html  # 加载到容器哪个目录
  volumeClaimTemplates: #数据卷模板
  - metadata:
      name: www
      annotations:  #数据卷注解
        volume.alpha.kubernetes.io/storage-class: anything
    spec: #数据卷的规约
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi #分配的空间

```

有状态服务的部署方案，实现即使删了pod，但是持久化的文件依然存在。

例如，多个nginx持久卷挂在一个文件下，即使这个pod被删掉，文件也不会被删掉。

![image-20240421174816078](https://raw.githubusercontent.com/PeipengWang/picture/master/image-20240421174816078.png)

### 3.2 创建

```
创建web服务
kubectl create -f web.yaml

# 查看 service 和 statefulset => sts
kubectl get service nginx
kubectl get statefulset web
# 进入容器进行文件修改
kubectl exec -it web-0 -- /bin/bash

# 查看 PVC 信息
kubectl get pvc

# 查看创建的 pod，这些 pod 是有序的
kubectl get pods -l app=nginx

```

### 3.4 扩容缩容

```
# 扩容
$ kubectl scale statefulset web --replicas=5

# 缩容
$ kubectl patch statefulset web -p '{"spec":{"replicas":3}}'

```

扩容缩容都可以商用上面两种方法

### 3.5 镜像更新 

**立即更新**

只有在 pod 被删除时会进行更新操作，不删除原来的pod，就不会更新，只有删除了pod，RS就会产生一个新的pod

**金丝雀发布**（**滚动更新**）

目标：将项目上线后产生的影响尽量降到最低

发布集群服务的时候先发布几个服务器上，在后面逐步滚动发布，只有已经更新的服务确认没有问题才会继续发布，不然就回滚。

核心需要解决的是应该发布代哪个节点上。

StatefulSet 也可以采用滚动更新策略，同样是修改 pod template 属性后会触发更新，但是由于 pod 是有序的，在 StatefulSet 中更新时是基于 pod 的顺序倒序更新的

利用滚动更新中的 partition 属性，可以实现简易的灰度发布的效果

例如我们有 5 个 pod，如果当前 partition 设置为 3，那么此时滚动更新时，只会更新那些 序号 >= 3 的 pod

利用该机制，我们可以通过控制 partition 的值，来决定只更新其中一部分 pod，确认没有问题后再主键增大更新的 pod 数量，最终实现全部 pod 更新

### 3.6 删除

删除 StatefulSet 和 Headless Service

级联删除：删除 statefulset 时会同时删除 pods

```
kubectl delete statefulset web
```

非级联删除：删除 statefulset 时不会删除 pods，删除 sts 后，pods 就没人管了，此时再删除 pod 不会重建的

```
kubectl deelte sts web --cascade=false
```

### 3.7 删除pvc

```
# StatefulSet删除后PVC还会保留着，数据不再使用的话也需要删除
$ kubectl delete pvc www-web-0 www-web-1
```



## 4 DaemonSet

为每一个匹配的Node部署一个后台pod，例如日志，监控

如下图所示，如果有三个节点，分别部署着不同的服务，如果出现问题了，就需要逐个检查各个服务节点的日志，这样会非常麻烦，因此如果加一个后台的日志服务，对日志进行统一收集，放到一个专门服务日志服务的节点，那么这个问题就迎刃而解了，这这个每个节点都有的pod就需要DaemonSet来定义。

![image-20240421200407467](https://raw.githubusercontent.com/PeipengWang/picture/master/image-20240421200407467.png)

### 4.1 模板

```
apiVersion: apps/v1
kind: DaemonSet # 创建 DaemonSet 资源
metadata:
  name: fluentd # 名字
spec:
  selector:
    matchLabels:
      app: logging
  template:
    metadata:
      labels:
        app: logging
        id: fluentd
      name: fluentd
    spec:
      containers:
      - name: fluentd-es
        image: agilestacks/fluentd-elasticsearch:v1.3.0
        env: # 环境变量配置
         - name: FLUENTD_ARGS # 环境变量的 key
           value: -qq # 环境变量的 value
        volumeMounts: # 加载数据卷，避免数据丢失
         - name: containers # 数据卷的名字
           mountPath: /var/lib/docker/containers # 将数据卷挂载到容器内的哪个目录
         - name: varlog
           mountPath: /varlog
      volumes: # 定义数据卷
         - hostPath: # 数据卷类型，主机路径的模式，也就是与 node 共享目录
             path: /var/lib/docker/containers # node 中的共享目录
           name: containers # 定义的数据卷的名称
         - hostPath:
             path: /var/log
           name: varlog

```

### 4.2 指定Node节点

#### 4.2.1 nodeSelector

先为 Node 打上标签
kubectl label nodes k8s-node1 svc_type=microsvc

然后再 daemonset 配置中设置 nodeSelector

```
spec:
  template:
    spec:
      nodeSelector:
        svc_type: microsvc
```

#### 4.2.2 n4odeAffinity

nodeAffinity 目前支持两种：requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution，分别代表必须满足条件和优选条件。

比如下面的例子代表调度到包含标签 wolfcode.cn/framework-name 并且值为 spring 或 springboot 的 Node 上，并且优选还带有标签 another-node-label-key=another-node-label-value 的Node。

```
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: wolfcode.cn/framework-name
            operator: In
            values:
            - spring
            - springboot
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: pauseyyf/pause
```

#### 4.2.3 podAffinity 

podAffinity 基于 Pod 的标签来选择 Node，仅调度到满足条件Pod 所在的 Node 上，支持 podAffinity 和 podAntiAffinity。这个功能比较绕，以下面的例子为例：

- 如果一个 “Node 所在空间中包含至少一个带有 auth=oauth2 标签且运行中的 Pod”，那么可以调度到该 Node
- 不调度到 “包含至少一个带有 auth=jwt 标签且运行中 Pod”的 Node 上

## 5 HPA

针对pod进行自动化的水平弹性收缩的能力

Pod 自动扩容：可以根据 CPU 使用率或自定义指标（metrics）自动对 Pod 进行扩/缩容。

- 控制管理器每隔30s（可以通过–horizontal-pod-autoscaler-sync-period修改）查询metrics的资源使用情况
- 支持三种metrics类型
  - 预定义metrics（比如Pod的CPU）以利用率的方式计算
  - 自定义的Pod metrics，以原始值（raw value）的方式计算
  - 自定义的object metrics
- 支持两种metrics查询方式：Heapster和自定义的REST API
- 支持多metrics

### 5.1 开启指标服务

下载 metrics-server 组件配置文件

```
wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml -O metrics-server-components.yaml

```

修改镜像地址为国内的地址

```
sed -i 's/k8s.gcr.io\/metrics-server/registry.cn-hangzhou.aliyuncs.com\/google_containers/g' metrics-server-components.yaml
```

修改容器的 tls 配置，不验证 tls，在 containers 的 args 参数中增加 --kubelet-insecure-tls 参数

安装组件

```
kubectl apply -f metrics-server-components.yaml
```

查看 pod 状态

```
kubectl get pods --all-namespaces | grep metrics
```

### 5.2 cpu、内存指标监控

```
实现 cpu 或内存的监控，首先有个前提条件是该对象必须配置了 resources.requests.cpu 或 resources.requests.memory 才可以，可以配置当 cpu/memory 达到上述配置的百分比后进行扩容或缩容

创建一个 HPA：
先准备一个好一个有做资源限制的 deployment
执行命令 kubectl autoscale deploy nginx-deploy --cpu-percent=20 --min=2 --max=5
通过 kubectl get hpa 可以获取 HPA 信息

测试：找到对应服务的 service，编写循环测试脚本提升内存与 cpu 负载
while true; do wget -q -O- http://<ip:port> > /dev/null ; done

可以通过多台机器执行上述命令，增加负载，当超过负载后可以查看 pods 的扩容情况 kubectl get pods

查看 pods 资源使用情况
kubectl top pods

扩容测试完成后，再关闭循环执行的指令，让 cpu 占用率降下来，然后过 5 分钟后查看自动缩容情况
```

### 5.3 自定义metrics

```
控制管理器开启–horizontal-pod-autoscaler-use-rest-clients
控制管理器的–apiserver指向API Server Aggregator
在API Server Aggregator中注册自定义的metrics API

```

